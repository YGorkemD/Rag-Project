import logging
from search import search_documents  
from model import generate_answer  

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def answer_query(user_query):  
    """KullanÄ±cÄ±nÄ±n sorusuna ChromaDB'den bilgi alÄ±p LLaMA modeli ile yanÄ±t Ã¼retir."""  
    logging.info(f"ğŸ” Sorgu alÄ±ndÄ±: {user_query}")  
    
    # ChromaDBâ€™den en alakalÄ± belgeleri getir
    docs, _ = search_documents(user_query)
    
    if not docs:
        logging.warning("âš ï¸ Aranan sorgu ile ilgili yeterli veri bulunamadÄ±.")
        return "ÃœzgÃ¼nÃ¼m, bu konuda yeterli veri bulunamadÄ±. LÃ¼tfen daha farklÄ± bir soru deneyin."
    
    # BaÄŸlamÄ± oluÅŸtur (En fazla 4000 karakter)
    context = " ".join(docs)[:4000]
    
    # Model iÃ§in uygun prompt hazÄ±rla
    final_prompt = f"Question: {user_query}\nContext: {context}\nAnswer:"
    
    # Modelden cevap al
    response = generate_answer(final_prompt)
    
    return response

if __name__ == "__main__":  
    logging.info("ğŸŸ¢ Sistem baÅŸlatÄ±ldÄ±. KullanÄ±cÄ± sorgularÄ± bekleniyor...")  

    while True:  
        user_query = input("\nEnter your question (or type 'exit' to quit): ").strip()
        
        if user_query.lower() == "exit":
            logging.info("ğŸ”´ Sistem durduruldu. Programdan Ã§Ä±kÄ±lÄ±yor...")
            print("Goodbye!")
            break
        
        response = answer_query(user_query)
        print("\nResponse:\n", response)
