import os
import torch
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Hugging Face Token kontrolÃ¼
HF_TOKEN = os.getenv("HF_TOKEN")  # Ã‡evre deÄŸiÅŸkeni olarak belirlenmiÅŸse al
if not HF_TOKEN:  
    HF_TOKEN = input("ğŸ”‘ LÃ¼tfen Hugging Face tokenÄ±nÄ±zÄ± girin: ").strip()

# Model ve cihaz ayarlarÄ±
MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Tokenizer ve model yÃ¼kleme
logging.info(f"ğŸ“¥ {MODEL_NAME} modeli yÃ¼kleniyor...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, 
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32, 
        device_map="auto", 
        token=HF_TOKEN
    )
    logging.info(f"âœ… {MODEL_NAME} baÅŸarÄ±yla yÃ¼klendi!")
except Exception as e:
    logging.error(f"âŒ Model yÃ¼klenirken hata oluÅŸtu: {e}")
    exit(1)  # Hata durumunda programdan Ã§Ä±k

def generate_answer(prompt, context=""):
    """LLM modelinden yanÄ±t Ã¼retir, main.py ile uyumlu hale getirildi."""
    final_prompt = f"Question: {prompt}\nContext: {context}\nAnswer:"
    inputs = tokenizer(final_prompt, return_tensors="pt").to(DEVICE)

    try:
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=50)  # ğŸ”¹ Token limiti 50 olarak bÄ±rakÄ±ldÄ±
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        logging.error(f"âŒ Model yanÄ±t Ã¼retemedi: {e}")
        response = "ÃœzgÃ¼nÃ¼m, yanÄ±t Ã¼retemedim. LÃ¼tfen daha kÄ±sa veya farklÄ± bir soru deneyin."

    return response
